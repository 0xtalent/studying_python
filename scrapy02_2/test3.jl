{"contents": ["The manual method of discovery for gauging online public sentiment towards a product, company, or industry is cursory at best, and at worst, may harm your business by providing incorrect or misleading insights.", "Simply put, this data can only be properly aggregated to any useful extent if generated in quantities too large for manual input, and web scraping such data has therefore become best-practice across the world\u2019s many public-facing industries.", "Sentiment analysis can transform the subjective emotions of the public into quantitative insight that a company or leader can use to drive change. Let's look at some popular use cases for online public sentiment data:", "The market moves quickly, and being even half a step ahead of the competition can be incredibly valuable. If many firms are receiving the same traditional data, investors can differentiate their strategy (and bolster their earnings) by incorporating sentiment data into their decision making. Sentiment data can inform the investment process by predicting future firm fundamentals or stock returns plus event-based sentiment analysis can show savvy investors when to move.", "Any business intelligence strategy is incomplete without an understanding of public sentiment. The way a company\u2019s product or service is portrayed in news articles and reviews directly impacts its bottom line. By scraping qualitative public sentiment data about your product from the web, businesses can integrate this information directly into AI-driven business solutions to produce actionable insights into which products are performing - or underperforming - and why. The uses for sentiment data across the product monitoring are diverse. Here are a few examples: polarity analysis, social media monitoring, aspect-based text analysis, and website variance.", "The adage that any press is good press may be losing its veracity in an era when press and reviews are at our fingertips 24/7. What steps can a company take to protect its reputation in this socially accelerating world? By scraping sentiment data, business intelligence (BI) teams can both capitalize on positive publicity and work to mitigate negative sentiments. A comprehensive BI strategy acknowledges the multi-pronged nature of online sentiment by scraping data from many sources and analyzing many of the factors that comprise sentiment.", "Sentiment analysis based on web scraped data equips product development teams with data-driven insights into the changes customers want to see and the quality of a product\u2019s performance. In this way, sentiment analysis is not just a retrospective but a vital tool in the product creation, planning, and design process. Get insights for every step of development from launching new features to customer feedback from support tickets and monitoring how customers are using your product to help determine new product creation or new features.", "Sentiment Analysis, with its useful analysis opens doors to so many opportunities by providing a solid scientific base to information that was previously mere assumptions. If you are curious about the above applications of sentiment analysis and want to know more, have a look at our whitepaper on \u2018", "\u2019. If you want to know how web scraped news and article data can benefit your business, request a ", " with our solution architecture team today."]}
{"contents": ["Product data - whether from e-commerce sites, auto listings or product reviews, offers a treasure trove of insights that can give your business an immense competitive edge in your market. Getting access to this data in a structured format can unleash new potential for not only business intelligence teams, but also their counterparts in marketing, sales, and management that rely on accurate data to make mission critical business decisions.", "At Scrapinghub, we have a unique view on how this data is used - we extract data from 9 billions web pages per month and can see firsthand how the world\u2019s most innovative companies are using product data extracted from the web to develop new business capabilities for themselves and their customers. Whether you\u2019re a hedge fund manager, start-up or an e-commerce giant, here are a few inspiring new uses for web scraped product data:", "To make profitable pricing decisions, having access to timely, reliable source of high quality data is crucial. By scraping pricing data, business intelligence teams are empowered to confidently position products and services. Conveniently, this data can be integrated directly into product management systems. Automated data collection leaves little scope for human error and enables an organization to monitor the whole market, and always stay a step ahead. Real-time ", " to stay on top of price changes has already proved to be so effective that it has become a must practise for top performing retail organizations.", "Using product data to track your competitors is as old as the internet itself. However, we are increasingly seeing companies completely redefining the scale and scope of their competitor monitoring capabilities through the use of big data, ", "and sophisticated machine learning technologies. Businesses are aware of the huge opportunities to generate actionable competitor insights by scraping product data and data teams have proliferated to meet the rising need. Powerful data extraction solutions enable companies to implement dynamic pricing and not miss market opportunities.", "Whether it\u2019s a long, slow shift or a trend cycle as rapid as those in fashion or technology, those who can see the farthest ahead - and most accurately - into future trends emerge on top. The scale and multidimensional aspect of product data makes it perfect for web scraping, and once integrated into product management systems, product data enables companies to see farther, predict more accurately, and drive revenue growth. Web scraped data empowering clothing makers to optimize their products, prices, trends, fashions and inventories to more accurately respond to seasonal and cyclical changes. This means high affinity with consumers, high revenue and bringing the right product to market at the right time.", "Hedge funds and account managers are increasingly incorporating ", "streams into their decision making processes, hoping to gain an information edge over the market and generate alpha. As a result, the demand for product data continues to surge, with a majority of institutional investors increasing the amount of web scraped data they consume. Given its predictive utility, it\u2019s not hard to see why product data is of huge interest to them. Today, web scraped big data, aided by artificial intelligence and machine learning allows firms to predict revenue momentum, stock prices, company valuations and identify risks and opportunities for investment and this is just the beginning!", "Access to accurate reliable product data empowers businesses to monitor every aspect of their omnichannel strategy. It\u2019s akin to being a fly on the wall of every authorized merchant, re-seller, and retail location selling a particular product, enabling quick turnaround on merchandising audits and MAP violations. In the past, this required constant vigilance and a lot of sunk time - today, software can automate these headaches."]}
{"contents": ["A huge portion of the internet is news. It\u2019s a very important type of content because there are always things happening either in our local area or globally that we want to know about. The amount of news published everyday on different sites is ridiculous. Sometimes it\u2019s good news and sometimes it\u2019s bad news but one thing\u2019s for sure: it\u2019s humanly impossible to read all of it everyday.", "In this article we take you through how to extract data from two popular news sites and perform basic exploratory analysis on the articles to find some overarching theme across news articles and maybe even news sites.", "Going through this article you will get some insights about our newest AI data extraction tool - ", ", and how it can be used to extract news data without writing any xpath or selectors.", "You will also learn some easy-to-do but spectacular methods to analyze text data. The exact process we are going to follow, for each news site:", "Discovering URLs on the site is an essential first step because otherwise we won\u2019t have the input data that is needed for the AutoExtract API. This input data is the URL of a news article. Nothing else is needed. So first, let\u2019s grab all the main page news article URLs. We need to setup a new Scrapy spider and use Scrapy\u2019s Linkextractor to find the correct URLs:", "Now that we\u2019ve got the URLs we can go ahead and use the AutoExtract API. These are the things you need to have, packaged in a JSON object, to make an API request:", "This is how the AutoExtract API call should look like, inside Scrapy:", "Adding this piece of code to our previously created spider:", "This function will first collect all the news article URLs on the main page using Scrapy\u2019s LinkExtractor. Then we pass each URL, one by one, to the AutoExtract API. AutoExtract will get all the data that is associated with the article, for example: article body, author(s), publish date, language and others. And the best part: without any html parsing or xpath."]}
{"contents": ["The Web Data Extraction Summit was held last week, on 17th September, in Dublin, Ireland. This was the first-ever event dedicated to web scraping and data extraction. We had over 140 curious attendees, 16 great speakers from technical deep dives to business use cases, 12 amazing presentations, a customer panel discussion and unlimited Guinness.", "Our goal with this event was to share not just our own expertise with the audience but also provide a platform for external industry experts and actual web data users to share their knowledge and experience with web data extraction. Based on the feedback we got during and after the event, it was an absolute success! The people who came learnt a lot and were able to connect with others in the industry.", "Here is a quick overview of the day and some highlights from the talks:", "To kick-off the day, Shane Evans, CEO at Scrapinghub talked about how web data extraction is impacting businesses and gave us his insights on how the industry will evolve.", "Right after, Cathal Garvey, Data Scientist, took the stage to provide some insights into what data types, use cases, applications and emerging trends. All these based on scraping 9 billion pages per month for thousands of Scrapinghub customers.", "The third talk was about legal compliance and GDPR in web scraping. Kate O\u2019Brien, Legal Counsel, talked about best practises guidelines to make sure you and your web scraper are compliant. The audience had quite a few questions about the ", " and others. It\u2019s obviously a hot topic nowadays.", "Attila Toth, Technology Evangelist and Pawel Miech, Technical Team Lead, talked about ", " and problems you need to solve if you extract data from the web at scale. Also providing actual tips for data extraction developers, like how to find data fields most effectively in an HTML or how to get through captchas. A takeaway from the presentation is that the web is like a jungle and you should not expect that each website will follow the standards.", "Then, Bryan O\u2019Brien, Product Manager, showed us how the new product from Scrapinghub "]}
{"contents": ["In this article I will guide you through a web scraping and data visualization project. We will extract e-commerce data from real e-commerce websites then try to get some insights out of it. The goal of this article is to show you how to get product pricing data from the web and what are some ways to analyze pricing data. We will also look at how price intelligence makes a real difference for e-commerce companies when making pricing decisions.", "This is the simple process we are going to follow for this article:", "In a real life project you\u2019d probably know which websites you want to get data from. For this article, I\u2019m choosing some popular European e-commerce stores.", "When scraping product information we have endless amount of data types we could get from an e-commerce site: product name, product specific attributes, price, stock, reviews, category etc. For now, we will focus on four fields that have the potential to give us the most interesting insights:", "Before we start writing code to extract data from any website, it\u2019s important to make sure we are scraping ethically. First, we should check the robots.txt file and see if it allows us to visit the pages we want to get data from.", "Example robots.txt:", "Some things you could do to be compliant:", "This is the part where we fetch the data from the website. We\u2019re going to use several modules of the Scrapy framework like Item, ItemLoader and pipeline. We want to make sure that the output is clean so we can insert it into a database for later analysis.", "We are using Scrapy, the web scraping framework for this project. It is recommended to install Scrapy in a virtual environment so it doesn\u2019t conflict with other system packages.", "Create a new folder and install virtualenv:"]}
{"contents": ["As you know we held the ", " last month. During the talks, we had a lot of questions from the audience. We have divided the questions into two parts - in the first part, we will cover questions on Web Scraping at Scale - Proxy and Anti-Ban Best Practice, and Legal Compliance, GDPR in the World of Web Scraping. Enjoy! You can also check out the full talks on these topics ", "A: As both antibots as well as bot developers have access to similar tools, there will be a constant ebb and flow and never a complete stop to web scraping.", "A: We handle all types of datacenter proxies from multiple providers to ensure a diverse pool for every use case. Our proxy pools are in the order of hundreds of thousands, while that's an important figure, we are constantly focusing on delivering successful responses to our customers.", "A: While Crawlera does provide browser profiles, going forward there will be more features built under the hood which will help with more sophisticated anti-bots.", "A: By crawling responsibly. Using ", ".", "A: We use headless browsers and all browsers can be detected as bots.", "A: It requires careful inspection of the response body, headers and at times the entire network traffic to understand the behaviour of the underlying anti-bot. We do use some internal tools to identify the type of detection used. There are also open source tools like \"don't fingerprint me\" which allow you to assess the browser fingerprinting used by the website.", "A: Yes they are and antibot companies do have a signature directory which is able to identify inconsistencies in the request headers."]}
{"contents": ["We\u2019ve just released a ", " which makes it easy to integrate AutoExtract into your existing Scrapy spider. If you haven\u2019t heard about ", " yet, it\u2019s an AI-based web scraping tool which automatically extracts data from web pages without the need to write any code. ", ".", "This project uses", " and", ". A", " is strongly encouraged.", "This middleware should be the last one to be executed so make sure to give it the highest value.", "These settings must be defined in order for AutoExtract to work."]}
{"contents": [", we answered some of the best questions we got during ", ". In today\u2019s post we share with you the second part of this series. We are covering questions on Web Scraping Infrastructure and How Machine Learning can be used in Web Scraping.", "Spider unit testing is difficult because it depends on the actual website which you don't have control over. You can use ", " for testing.", "Currently, the only tool that is specifically designed for web scraping and to render javascript is ", ". Another option can be to use a headless browser like selenium.", "In ", " we use NoSQL database for data storage. One of the advantages of NoSQL is the lack of schema which means you can change Scrapy item definition at any time without any problems.", "It depends on the project. In some projects there is a dedicated QA team that checks data, in some projects there is no such need. Sometimes it is enough to add automated checks for typical problems, and only do data checks after some major spider update.", "It depends on the client\u2019s needs. If they need a screenshot of some page as the user sees it they will always need javascript rendering. If they need some content from the website, it is difficult or may be impossible to detect which site needs javascript rendering and which don't."]}
{"contents": ["In today\u2019s article we will extract real estate listings from one of the biggest real estate sites and then analyze the data. Similar to our previous ", ", I will show you a simple way to extract web data with python and then perform descriptive analysis on the dataset.", "We are going to use python as a programming language.", "Tools and libraries:", "Although this could be a really complex project as it involves web scraping and data analysis as well, we are going to make it simple by using this process:", "Let\u2019s start!", "For every web scraping project the first question we need to answer is this - What data do we exactly need? When it comes to real-estate listings, there are so many data points we could scrape that we would have to really narrow them down based on our needs. For now, I\u2019m going to choose these fields:", "These data fields will give us freedom to look at the listings from different perspectives.", "Now that we know what data to extract from the website we can start working on our spider.", "We are using Scrapy, the web scraping framework for this project. It is recommended to install Scrapy in a virtual environment so it doesn\u2019t conflict with other system packages."]}
{"contents": [" a ", ", specifically designed for web scraping. In this article, you are going to learn how to use Crawlera inside your Scrapy spider.", "Crawlera is a smart HTTP/HTTPS downloader. When you make requests using Crawlera it routes them through a pool of IP addresses. When necessary, it automatically introduces delays between requests and discards IP addresses to avoid anti-crawling measures. And simply like that, it makes a successful request hassle-free.", "In order to use Crawlera you need to have an account with ", ". If you haven\u2019t signed up yet you can ", ", it\u2019s free. When you subscribe to a plan you will get an API key. You will need to use this API key in your Scrapy project to use Crawlera.", "First thing you need to do is to install the Crawlera middleware:", "Next, add these lines to the project settings:", "By using the middleware you add ", " to your project that you can configure. These settings can be overridden in Scrapy settings. For example it\u2019s recommended to set these:"]}
{"contents": "The manual method of discovery for gauging online public sentiment towards a product, company, or industry is cursory at best, and at worst, may harm your business by providing incorrect or misleading insights.Simply put, this data can only be properly aggregated to any useful extent if generated in quantities too large for manual input, and web scraping such data has therefore become best-practice across the world\u2019s many public-facing industries.Sentiment analysis can transform the subjective emotions of the public into quantitative insight that a company or leader can use to drive change. Let's look at some popular use cases for online public sentiment data:The market moves quickly, and being even half a step ahead of the competition can be incredibly valuable. If many firms are receiving the same traditional data, investors can differentiate their strategy (and bolster their earnings) by incorporating sentiment data into their decision making. Sentiment data can inform the investment process by predicting future firm fundamentals or stock returns plus event-based sentiment analysis can show savvy investors when to move.Any business intelligence strategy is incomplete without an understanding of public sentiment. The way a company\u2019s product or service is portrayed in news articles and reviews directly impacts its bottom line. By scraping qualitative public sentiment data about your product from the web, businesses can integrate this information directly into AI-driven business solutions to produce actionable insights into which products are performing - or underperforming - and why. The uses for sentiment data across the product monitoring are diverse. Here are a few examples: polarity analysis, social media monitoring, aspect-based text analysis, and website variance."}
{"contents": "Product data - whether from e-commerce sites, auto listings or product reviews, offers a treasure trove of insights that can give your business an immense competitive edge in your market. Getting access to this data in a structured format can unleash new potential for not only business intelligence teams, but also their counterparts in marketing, sales, and management that rely on accurate data to make mission critical business decisions.At Scrapinghub, we have a unique view on how this data is used - we extract data from 9 billions web pages per month and can see firsthand how the world\u2019s most innovative companies are using product data extracted from the web to develop new business capabilities for themselves and their customers. Whether you\u2019re a hedge fund manager, start-up or an e-commerce giant, here are a few inspiring new uses for web scraped product data:To make profitable pricing decisions, having access to timely, reliable source of high quality data is crucial. By scraping pricing data, business intelligence teams are empowered to confidently position products and services. Conveniently, this data can be integrated directly into product management systems. Automated data collection leaves little scope for human error and enables an organization to monitor the whole market, and always stay a step ahead. Real-time  to stay on top of price changes has already proved to be so effective that it has become a must practise for top performing retail organizations.Using product data to track your competitors is as old as the internet itself. However, we are increasingly seeing companies completely redefining the scale and scope of their competitor monitoring capabilities through the use of big data, "}
{"contents": "A huge portion of the internet is news. It\u2019s a very important type of content because there are always things happening either in our local area or globally that we want to know about. The amount of news published everyday on different sites is ridiculous. Sometimes it\u2019s good news and sometimes it\u2019s bad news but one thing\u2019s for sure: it\u2019s humanly impossible to read all of it everyday.In this article we take you through how to extract data from two popular news sites and perform basic exploratory analysis on the articles to find some overarching theme across news articles and maybe even news sites.Going through this article you will get some insights about our newest AI data extraction tool - , and how it can be used to extract news data without writing any xpath or selectors.You will also learn some easy-to-do but spectacular methods to analyze text data. The exact process we are going to follow, for each news site:"}
{"contents": "The Web Data Extraction Summit was held last week, on 17th September, in Dublin, Ireland. This was the first-ever event dedicated to web scraping and data extraction. We had over 140 curious attendees, 16 great speakers from technical deep dives to business use cases, 12 amazing presentations, a customer panel discussion and unlimited Guinness.Our goal with this event was to share not just our own expertise with the audience but also provide a platform for external industry experts and actual web data users to share their knowledge and experience with web data extraction. Based on the feedback we got during and after the event, it was an absolute success! The people who came learnt a lot and were able to connect with others in the industry.Here is a quick overview of the day and some highlights from the talks:To kick-off the day, Shane Evans, CEO at Scrapinghub talked about how web data extraction is impacting businesses and gave us his insights on how the industry will evolve.Right after, Cathal Garvey, Data Scientist, took the stage to provide some insights into what data types, use cases, applications and emerging trends. All these based on scraping 9 billion pages per month for thousands of Scrapinghub customers."}
{"contents": "In this article I will guide you through a web scraping and data visualization project. We will extract e-commerce data from real e-commerce websites then try to get some insights out of it. The goal of this article is to show you how to get product pricing data from the web and what are some ways to analyze pricing data. We will also look at how price intelligence makes a real difference for e-commerce companies when making pricing decisions.This is the simple process we are going to follow for this article:In a real life project you\u2019d probably know which websites you want to get data from. For this article, I\u2019m choosing some popular European e-commerce stores.When scraping product information we have endless amount of data types we could get from an e-commerce site: product name, product specific attributes, price, stock, reviews, category etc. For now, we will focus on four fields that have the potential to give us the most interesting insights:Before we start writing code to extract data from any website, it\u2019s important to make sure we are scraping ethically. First, we should check the robots.txt file and see if it allows us to visit the pages we want to get data from."}
{"contents": "As you know we held the  last month. During the talks, we had a lot of questions from the audience. We have divided the questions into two parts - in the first part, we will cover questions on Web Scraping at Scale - Proxy and Anti-Ban Best Practice, and Legal Compliance, GDPR in the World of Web Scraping. Enjoy! You can also check out the full talks on these topics A: As both antibots as well as bot developers have access to similar tools, there will be a constant ebb and flow and never a complete stop to web scraping.A: We handle all types of datacenter proxies from multiple providers to ensure a diverse pool for every use case. Our proxy pools are in the order of hundreds of thousands, while that's an important figure, we are constantly focusing on delivering successful responses to our customers.A: While Crawlera does provide browser profiles, going forward there will be more features built under the hood which will help with more sophisticated anti-bots."}
{"contents": "We\u2019ve just released a  which makes it easy to integrate AutoExtract into your existing Scrapy spider. If you haven\u2019t heard about  yet, it\u2019s an AI-based web scraping tool which automatically extracts data from web pages without the need to write any code. .This project uses"}
{"contents": ", we answered some of the best questions we got during . In today\u2019s post we share with you the second part of this series. We are covering questions on Web Scraping Infrastructure and How Machine Learning can be used in Web Scraping.Spider unit testing is difficult because it depends on the actual website which you don't have control over. You can use  for testing.Currently, the only tool that is specifically designed for web scraping and to render javascript is "}
{"contents": "In today\u2019s article we will extract real estate listings from one of the biggest real estate sites and then analyze the data. Similar to our previous , I will show you a simple way to extract web data with python and then perform descriptive analysis on the dataset.We are going to use python as a programming language.Tools and libraries:Although this could be a really complex project as it involves web scraping and data analysis as well, we are going to make it simple by using this process:"}
{"contents": " a , specifically designed for web scraping. In this article, you are going to learn how to use Crawlera inside your Scrapy spider.Crawlera is a smart HTTP/HTTPS downloader. When you make requests using Crawlera it routes them through a pool of IP addresses. When necessary, it automatically introduces delays between requests and discards IP addresses to avoid anti-crawling measures. And simply like that, it makes a successful request hassle-free.In order to use Crawlera you need to have an account with . If you haven\u2019t signed up yet you can "}
